{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo sistema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from plots import plot_tanks\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constantes\n",
    "a1 = 50  # cm^2\n",
    "a2 = 50  # cm^2\n",
    "cv1 = 1\n",
    "cv2 = 1\n",
    "\n",
    "\n",
    "def F(t, op=np):\n",
    "    f = 5 - (t**2) / 3000\n",
    "    return op.where(f < 0.0, 0.0, f)\n",
    "\n",
    "\n",
    "def edo_np(t, Y):\n",
    "    h1, h2 = Y[0], Y[1]\n",
    "\n",
    "    # Tanks limits\n",
    "    h1 = np.where(h1 < 0.0, 0.0, h1)\n",
    "    h2 = np.where(h2 < 0.0, 0.0, h2)\n",
    "\n",
    "    # Equations\n",
    "    dh1dt = (F(t) - cv1 * np.sqrt(h1)) / a1\n",
    "    dh2dt = (cv1 * np.sqrt(h1) - cv2 * np.sqrt(h2)) / a2\n",
    "    return np.array([dh1dt, dh2dt])\n",
    "\n",
    "\n",
    "t = np.linspace(0, 300, 300)\n",
    "t_tensor = torch.tensor(t, dtype=torch.float32, requires_grad=True).unsqueeze(1)\n",
    "y0 = np.array([0, 0])\n",
    "\n",
    "sol = solve_ivp(edo_np, [t[0], t[-1]], y0, t_eval=t)\n",
    "h1_exp = sol.y[0] + np.random.normal(0, 1, len(t)) * 0.1\n",
    "h2_exp = sol.y[1] + np.random.normal(0, 1, len(t)) * 0.1\n",
    "\n",
    "plot_tanks(\n",
    "    t, [h1_exp, h2_exp], [\"h1 (exp)\", \"h2 (exp)\"], scatter=2, filename=\"exp_tanks\"\n",
    ")\n",
    "\n",
    "# plt.figure(figsize=(10, 4), layout=\"constrained\")\n",
    "# plt.plot(t, F(t), label=\"F\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo rede neural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_size = 1  # t\n",
    "        hidden_size = 8\n",
    "        output_size = 2  # h1, h2\n",
    "\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.hidden_layer((x * 2 / torch.max(t_tensor)) - 1)\n",
    "        return torch.abs((output + 1) * (torch.max(h1_exp) / 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo função Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dydx, mean_square\n",
    "\n",
    "h1_exp = torch.tensor(h1_exp, dtype=torch.float32)\n",
    "h2_exp = torch.tensor(h2_exp, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def edo_torch(t, Y):\n",
    "    h1, h2 = Y\n",
    "\n",
    "    # Equations\n",
    "    dh1dt = (F(t, torch) - cv1 * torch.sqrt(h1)) / a1\n",
    "    dh2dt = (cv1 * torch.sqrt(h1) - cv2 * torch.sqrt(h2)) / a2\n",
    "    return [dh1dt, dh2dt]\n",
    "\n",
    "\n",
    "def loss_fn(model, t):\n",
    "    # Loss das EDOs\n",
    "    Y_pred = model(t)\n",
    "    h1_pred, h2_pred = Y_pred[:, 0], Y_pred[:, 1]\n",
    "\n",
    "    dh1dt_pinn, dh2dt_pinn = dydx(t, h1_pred), dydx(t, h2_pred)\n",
    "    dh1dt_edo, dh2dt_edo = edo_torch(t, [h1_pred, h2_pred])\n",
    "\n",
    "    loss_EDO1 = mean_square(dh1dt_pinn - dh1dt_edo)\n",
    "    loss_EDO2 = mean_square(dh2dt_pinn - dh2dt_edo)\n",
    "\n",
    "    # Loss das condições iniciais\n",
    "    t0 = torch.tensor([[0.0]], requires_grad=True)\n",
    "    Y0 = model(t0)\n",
    "    h1_0, h2_0 = Y0[:, 0], Y0[:, 1]\n",
    "\n",
    "    loss_ic1 = mean_square(h1_0 - y0[0])\n",
    "    loss_ic2 = mean_square(h2_0 - y0[1])\n",
    "\n",
    "    # Loss dos dados\n",
    "    loss_data_h1 = mean_square(h1_pred - h1_exp)\n",
    "    loss_data_h2 = mean_square(h2_pred - h2_exp)\n",
    "\n",
    "    # Loss total\n",
    "    loss_total = (\n",
    "        loss_EDO1 + loss_EDO2 + loss_data_h1 + loss_data_h2 + loss_ic1 + loss_ic2\n",
    "    )\n",
    "\n",
    "    return loss_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando métodos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_execuções = 2\n",
    "target_loss = 0.1\n",
    "\n",
    "Adam_study_path = \"../results/Adam-studies.hkl\"\n",
    "Adam_results_path = \"../results/Adam-speeds.hkl\"\n",
    "Adam_model_path = \"../results/Adam-model.pt\"\n",
    "\n",
    "GA_study_path = \"../results/GA-studies.hkl\"\n",
    "GA_results_path = \"../results/GA-speeds.hkl\"\n",
    "GA_model_path = \"../results/GA-model.pt\"\n",
    "\n",
    "\n",
    "def count_fails(losses):\n",
    "    return np.sum(np.array(losses) > target_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizando hiperparametros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:23:57,169] A new study created in memory with name: Adam-study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperando estudo dos hiperparametros do Adam.\n",
      "Melhor Adam:\n",
      "  Valor do Loss: 0.03868981450796127\n",
      "  hiperparametros:\n",
      "    lr: 0.003703217091831923\n",
      "    beta1: 0.8559747074607573\n",
      "    beta2: 0.39993555192502994\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import hickle as hkl\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    torch.manual_seed(42)\n",
    "    test_model = BaseModel()\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-15, 1)\n",
    "    beta1 = trial.suggest_float(\"beta1\", 1e-10, 1)\n",
    "    beta2 = trial.suggest_float(\"beta2\", 1e-10, 1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(test_model.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    # O Loop de treinamento\n",
    "    for _ in range(1000):\n",
    "        # Coloca o modelo no modo de treinamento\n",
    "        test_model.train()\n",
    "\n",
    "        # Calcula o loss usando a nossa função loss.\n",
    "        loss = loss_fn(test_model, t_tensor)\n",
    "\n",
    "        # Ajusta os valores do modelo\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return float(loss_fn(test_model, t_tensor).detach().numpy())\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"Adam-study\")\n",
    "\n",
    "if not os.path.exists(Adam_study_path):\n",
    "    print(\"Otimizando hiperparametros do Adam\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(\"Salvando estudo para futuras execuções\")\n",
    "    hkl.dump(study, Adam_study_path)\n",
    "else:\n",
    "    print(\"Recuperando estudo dos hiperparametros do Adam.\")\n",
    "    study = hkl.load(Adam_study_path)\n",
    "\n",
    "best_Adam = study.best_trial\n",
    "print(\"Melhor Adam:\")\n",
    "print(\"  Valor do Loss:\", best_Adam.value)\n",
    "\n",
    "print(\"  hiperparametros:\")\n",
    "for key, value in best_Adam.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medindo desempenho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperando testes anteriores\n",
      "Média do tempo (Adam): 2.116s\n",
      "Tentativas falhadas: 0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "Adam_times = []\n",
    "Adam_losses = []\n",
    "Adam_model = BaseModel()\n",
    "\n",
    "if not (os.path.exists(Adam_results_path) and os.path.exists(Adam_model_path)):\n",
    "    print(\"Fazendo testes de desempenho do Adam\")\n",
    "    for i in range(n_execuções):\n",
    "        start_time = time.monotonic()\n",
    "        torch.manual_seed(i)\n",
    "        Adam_model = BaseModel()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            Adam_model.parameters(),\n",
    "            lr=best_Adam.params[\"lr\"],\n",
    "            betas=(best_Adam.params[\"beta1\"], best_Adam.params[\"beta2\"]),\n",
    "        )\n",
    "\n",
    "        loss_value = 0\n",
    "        for epoch in range(5000):\n",
    "            # Coloca o modelo no modo de treinamento\n",
    "            Adam_model.train()\n",
    "\n",
    "            # Calcula o loss usando a nossa função loss.\n",
    "            loss = loss_fn(Adam_model, t_tensor)\n",
    "\n",
    "            # Ajusta os valores do modelo\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_value = float(loss.detach().numpy())\n",
    "            if loss_value < target_loss:\n",
    "                break\n",
    "\n",
    "        elapsed_time = time.monotonic() - start_time\n",
    "        Adam_times.append(elapsed_time)\n",
    "        Adam_losses.append(loss_value)\n",
    "    print(\"Salvando para futuras execuções\")\n",
    "    hkl.dump((Adam_times, Adam_losses), Adam_results_path)\n",
    "    Adam_model.eval()\n",
    "    torch.save(Adam_model.state_dict(), Adam_model_path)\n",
    "else:\n",
    "    print(\"Recuperando testes anteriores\")\n",
    "    Adam_times, Adam_losses = hkl.load(Adam_results_path)\n",
    "    Adam_model.load_state_dict(torch.load(Adam_model_path, weights_only=True))\n",
    "\n",
    "print(f\"Média do tempo (Adam): {np.mean(Adam_times):.3f}s\")\n",
    "print(\"Tentativas falhadas:\", count_fails(Adam_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def test_model(model):\n",
    "    y = model(t_tensor)\n",
    "    return [y[:, 0], y[:, 1]]\n",
    "\n",
    "\n",
    "pinn_h1, pinn_h2 = test_model(Adam_model)\n",
    "\n",
    "# Gráfico\n",
    "plot_tanks(\n",
    "    t,\n",
    "    (h1_exp, h2_exp, pinn_h1, pinn_h2),\n",
    "    [\"h1 (exp)\", \"h2 (exp)\", \"h1 (PINN Adam)\", \"h2 (PINN Adam)\"],\n",
    "    scatter=2,\n",
    "    filename=\"adam_tanks\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Genético\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "from pygad.torchga import torchga\n",
    "\n",
    "GA_model = BaseModel()\n",
    "\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    model_weights_dict = torchga.model_weights_as_dict(\n",
    "        model=GA_model, weights_vector=solution\n",
    "    )\n",
    "    GA_model.load_state_dict(model_weights_dict)\n",
    "\n",
    "    GA_model.eval()\n",
    "    loss = loss_fn(GA_model, t_tensor)\n",
    "\n",
    "    # Quanto menor o loss, maior o fitness\n",
    "    return -loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizando hiperparametros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:23:57,534] A new study created in memory with name: GA-study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperando estudo dos hiperparametros do Algoritmo Genético.\n",
      "Melhor GA:\n",
      "  Valor do Loss: -0.2702963352203369\n",
      "  hiperparametros:\n",
      "    parent_selection_type: tournament\n",
      "    keep_elitism: 8\n",
      "    num_parents_mating: 4\n",
      "    crossover_type: None\n",
      "    crossover_probability: 0.27540356384223963\n",
      "    mutation_type: adaptive\n",
      "    mutation_probability: 0.3874911981463413\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    torch.manual_seed(42)\n",
    "    test_model = BaseModel()\n",
    "\n",
    "    parent_selection_type = trial.suggest_categorical(\n",
    "        \"parent_selection_type\", [\"sss\", \"rws\", \"sus\", \"rank\", \"random\", \"tournament\"]\n",
    "    )\n",
    "    keep_elitism = trial.suggest_int(\"keep_elitism\", 0, 10)\n",
    "    num_parents_mating = trial.suggest_int(\"num_parents_mating\", 2, 10)\n",
    "\n",
    "    crossover_type = trial.suggest_categorical(\n",
    "        \"crossover_type\", [\"single_point\", \"two_points\", \"uniform\", \"scattered\", None]\n",
    "    )\n",
    "    crossover_probability = trial.suggest_float(\"crossover_probability\", 0, 1)\n",
    "\n",
    "    mutation_type = trial.suggest_categorical(\n",
    "        \"mutation_type\", [\"random\", \"swap\", \"inversion\", \"scramble\", \"adaptive\", None]\n",
    "    )\n",
    "    mutation_probability = trial.suggest_float(\"mutation_probability\", 0, 1)\n",
    "\n",
    "    # Configura o TorchGA para criar populações baseadas no modelo\n",
    "    torch_ga = torchga.TorchGA(model=test_model, num_solutions=50)\n",
    "\n",
    "    # Configura o algoritmo genético\n",
    "    ga_instance = pygad.GA(\n",
    "        # Configurações\n",
    "        initial_population=torch_ga.population_weights,  # População inicial\n",
    "        fitness_func=fitness_func,  # Função de aptidão\n",
    "        num_generations=80,  # Número de gerações\n",
    "        random_seed=42,\n",
    "        init_range_low=-4,\n",
    "        init_range_high=4,\n",
    "        # Parâmetros para otimizar\n",
    "        parent_selection_type=parent_selection_type,\n",
    "        keep_elitism=keep_elitism,\n",
    "        num_parents_mating=num_parents_mating,\n",
    "        crossover_type=crossover_type,  # type: ignore\n",
    "        crossover_probability=crossover_probability,\n",
    "        mutation_type=mutation_type,  # type: ignore\n",
    "        mutation_probability=(\n",
    "            mutation_probability if mutation_type != \"adaptive\" else [0.8, 0.1]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Executa o algoritmo genético\n",
    "    ga_instance.run()\n",
    "\n",
    "    _, best_solution_fitness, _ = ga_instance.best_solution()\n",
    "\n",
    "    return float(best_solution_fitness)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"GA-study\")\n",
    "\n",
    "if not os.path.exists(GA_study_path):\n",
    "    print(\"Otimizando hiperparametros do Algoritmo Genético\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(\"Salvando estudo para futuras execuções\")\n",
    "    hkl.dump(study, GA_study_path)\n",
    "else:\n",
    "    print(\"Recuperando estudo dos hiperparametros do Algoritmo Genético.\")\n",
    "    study = hkl.load(GA_study_path)\n",
    "\n",
    "best_GA = study.best_trial\n",
    "print(\"Melhor GA:\")\n",
    "print(\"  Valor do Loss:\", best_GA.value)\n",
    "\n",
    "print(\"  hiperparametros:\")\n",
    "for key, value in best_GA.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medindo desempenho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperando testes anteriores\n",
      "Média do tempo (GA): 45.804s\n",
      "Tentativas falhadas: 2\n"
     ]
    }
   ],
   "source": [
    "GA_times = []\n",
    "GA_losses = []\n",
    "GA_model = BaseModel()\n",
    "\n",
    "\n",
    "def on_generation(ga_instance):\n",
    "    if ga_instance.best_solution()[1] > -target_loss:\n",
    "        return \"stop\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "if best_GA.params[\"mutation_type\"] == \"adaptive\":\n",
    "    best_GA.params[\"mutation_probability\"] = [0.8, 0.1]\n",
    "\n",
    "if not (os.path.exists(GA_results_path) and os.path.exists(GA_model_path)):\n",
    "    print(\"Fazendo testes de desempenho do GA\")\n",
    "    for i in range(n_execuções):\n",
    "        start_time = time.monotonic()\n",
    "        torch.manual_seed(i)\n",
    "        GA_model = BaseModel()\n",
    "\n",
    "        torch_ga = torchga.TorchGA(model=GA_model, num_solutions=100)\n",
    "        ga_instance = pygad.GA(\n",
    "            initial_population=torch_ga.population_weights,\n",
    "            fitness_func=fitness_func,\n",
    "            random_seed=i,\n",
    "            num_generations=50,\n",
    "            init_range_low=-4,\n",
    "            init_range_high=4,\n",
    "            on_generation=on_generation,\n",
    "            **best_GA.params,\n",
    "        )\n",
    "        ga_instance.run()\n",
    "        best_solution, best_solution_fitness, _ = ga_instance.best_solution()\n",
    "\n",
    "        elapsed_time = time.monotonic() - start_time\n",
    "        GA_times.append(elapsed_time)\n",
    "        GA_losses.append(-best_solution_fitness)\n",
    "\n",
    "        model_weights_dict = torchga.model_weights_as_dict(\n",
    "            model=GA_model, weights_vector=best_solution\n",
    "        )\n",
    "        GA_model.load_state_dict(model_weights_dict)\n",
    "\n",
    "    print(\"Salvando para futuras execuções\")\n",
    "    hkl.dump((GA_times, GA_losses), GA_results_path)\n",
    "    GA_model.eval()\n",
    "    torch.save(GA_model.state_dict(), GA_model_path)\n",
    "else:\n",
    "    print(\"Recuperando testes anteriores\")\n",
    "    GA_times, GA_losses = hkl.load(GA_results_path)\n",
    "    GA_model.load_state_dict(torch.load(GA_model_path, weights_only=True))\n",
    "\n",
    "print(f\"Média do tempo (GA): {np.mean(GA_times):.3f}s\")\n",
    "print(\"Tentativas falhadas:\", count_fails(GA_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_h1, pinn_h2 = test_model(GA_model)\n",
    "\n",
    "# Gráfico\n",
    "plot_tanks(\n",
    "    t,\n",
    "    (h1_exp, h2_exp, pinn_h1, pinn_h2),\n",
    "    [\"h1 (exp)\", \"h2 (exp)\", \"h1 (PINN GA)\", \"h2 (PINN GA)\"],\n",
    "    scatter=2,\n",
    "    filename=\"ga_tanks\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome: hidden_layer.0.weight\n",
      "Valor: Parameter containing:\n",
      "tensor([[ 2.1858],\n",
      "        [-0.1780],\n",
      "        [-1.6431],\n",
      "        [ 1.0773],\n",
      "        [-1.2982],\n",
      "        [ 1.0562],\n",
      "        [-1.3724],\n",
      "        [ 1.2637]], requires_grad=True)\n",
      "Nome: hidden_layer.0.bias\n",
      "Valor: Parameter containing:\n",
      "tensor([ 1.8094,  0.2047,  0.3160, -0.0519,  2.0033, -0.9800, -1.3178, -0.1154],\n",
      "       requires_grad=True)\n",
      "Nome: hidden_layer.2.weight\n",
      "Valor: Parameter containing:\n",
      "tensor([[-0.0243, -2.4715, -0.2404, -0.8101, -0.8423, -1.3759, -1.6582, -0.2309],\n",
      "        [-0.6367,  0.7258,  0.0584, -0.3996, -0.6812,  1.3138,  2.0029,  1.4318],\n",
      "        [ 1.2525, -1.3153, -2.0236,  0.6997, -0.5913,  1.9079, -0.6822, -0.9790],\n",
      "        [-0.4022, -0.9995,  1.9338, -0.5817,  0.7677, -1.2706,  0.2724, -0.4656],\n",
      "        [ 0.9965,  0.3907, -1.5333,  0.0191,  0.5278, -0.2682, -0.3564,  0.7896],\n",
      "        [-0.1341, -2.2246, -0.4204, -0.0642,  0.3585, -1.3890,  1.0711,  0.2997],\n",
      "        [-0.2110,  0.9569,  0.3655, -0.3288, -0.6878,  0.3509,  0.4450,  0.5238],\n",
      "        [ 0.3060,  0.2246, -0.5446,  0.5898, -0.8920, -0.7668, -0.1293,  1.2701]],\n",
      "       requires_grad=True)\n",
      "Nome: hidden_layer.2.bias\n",
      "Valor: Parameter containing:\n",
      "tensor([ 0.4158, -0.9896,  0.4625, -0.1992,  1.1512, -0.8494, -2.2680, -0.7055],\n",
      "       requires_grad=True)\n",
      "Nome: hidden_layer.4.weight\n",
      "Valor: Parameter containing:\n",
      "tensor([[-0.1439,  0.6911,  0.1195, -0.2632, -0.9282,  0.0693, -0.5349,  0.6374],\n",
      "        [-0.4978,  1.5134, -0.8217, -0.7113, -0.1328,  0.2115, -0.1204,  0.0718]],\n",
      "       requires_grad=True)\n",
      "Nome: hidden_layer.4.bias\n",
      "Valor: Parameter containing:\n",
      "tensor([-0.4858,  0.6396], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for nome, param in GA_model.named_parameters():\n",
    "    print(f\"Nome: {nome}\")\n",
    "    print(f\"Valor: {param}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
